{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![PPGI_UFRJ](imagens/ppgi-ufrj.png)\n",
    "# Fundamentos de Ciência de Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "[![DOI](https://zenodo.org/badge/335308405.svg)](https://zenodo.org/badge/latestdoi/335308405)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# PPGI/UFRJ 2020.3\n",
    "## Prof Sergio Serra e Jorge Zavaleta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Módulo 4. Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow 2.0\n",
    "> Um **tensor** é um objeto matemático e uma generalização de escalares, vetores e matrizes. Um tensor pode ser representado como uma matriz multidimensional.\n",
    "\n",
    "> Um tensor de ordem zero (rank) é um escalar. Um vetor / matriz é um tensor de ordem 1, enquanto uma matriz é um tensor de ordem 2. Em consequência, um tensor pode ser considerado uma matriz n-dimensional.\n",
    "\n",
    "> Exemplos de tensores:\n",
    "> - 7 : é um tensor de ordem 0. Escalar com shape [].\n",
    "> - [3.,5, 13]: Tensor de ordem 1. É um vetor com shape [3]\n",
    "> - [[10,4,6],[3,4,5]]: É um tensor de ordem 2. É uma matriz com shpe [2,3]\n",
    "> - [[[17,5,1]],[[9,6,3]]]: É um tensor de ordem 3 com shape [2,1,3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grafo Computacional e Session v1.0\n",
    "> O **tensorflow** tem dois programas básico que formam parte do kernel e executam duas ações principais:\n",
    "> - Construir um grafo computacional na fase inicial (fase de construção)\n",
    "> - Rodar ou executar o grafo computacional na fase de execução.\n",
    "\n",
    "> Um **grafo computacional** é uma série de operações do TensorFlow organizadas em nós de um gráfo.\n",
    "\n",
    "> Em **TensorFlow**, se pode configurar um grafo (um grafo padrão). Em seguida, se precisa criar variáveis, marcadores de posição e  valores constantes e, em seguida, se cria a sessão e inicializa as variáveis. Finalmente, se alimenta com os dados os marcadores de posição, de modo a invocar qualquer ação.\n",
    "\n",
    "> Para finalmente avaliar os nós, se deve executar o grafo computacional em uma **sessão**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras\n",
    "> **Keras** é uma API de deep Learning escrita em Python, executada em cima de **TensorFlow**. Foi projetada para permitir experimentação rápida em redes neurais profundas. [Keras](https://keras.io/)\n",
    "\n",
    "> A estrurura de dados básica do Keras são as **camadas** (layer) e os **modelos** (models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelo sequencial\n",
    "from tensorflow.keras.models import Sequential\n",
    "#\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empilhando camadas: add()\n",
    "from tensorflow.keras.layers import Dense\n",
    "#\n",
    "model.add(Dense(units=64, activation='relu'))\n",
    "model.add(Dense(units=10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criar o modelo e configurar: .comple()\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. importar as bibliotecas\n",
    "import tensorflow as tf\n",
    "#\n",
    "print('tf:',tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Definição de Constantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definição de constantes\n",
    "c1 = tf.constant(6.3)\n",
    "c2 = tf.constant(5.0)\n",
    "c3 = tf.constant(7.0)\n",
    "print(c1,c2,c3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">### Operações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Operations\n",
    "r1 = tf.add(c2,c3)                    # soma\n",
    "print('soma:',r1)\n",
    "r2 = tf.add(c2,-c3)                   # substração\n",
    "print('subtração:',r2)\n",
    "#\n",
    "op2=tf.multiply(c2,c3)               # multiplicação\n",
    "print('Multiplicação:',op2)\n",
    "op2=tf.math.divide(c2,c3)            # divide\n",
    "print('Divisão:',op2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# equação  y = ax + b\n",
    "a = tf.constant([[10,10],[11.,1.]])\n",
    "x = tf.constant([[1.,0.],[0.,1.]])\n",
    "b = tf.Variable(12.)\n",
    "y = tf.matmul(a, x) + b\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regressão Linear Usando TensorFlow e Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importar as bilbliotecas\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "#import tensorflow as tf\n",
    "from tensorflow import keras as ks\n",
    "from tensorflow.estimator import LinearRegressor\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset boston\n",
    "boston_load = datasets.load_boston()\n",
    "feature_columns = boston_load.feature_names                  \n",
    "boston_data = pd.DataFrame(boston_load.data,columns=feature_columns).astype(np.float32)\n",
    "boston_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target column\n",
    "target_column = boston_load.target\n",
    "print(target_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_data['MEDV'] = target_column.astype(np.float32)\n",
    "boston_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. relações entre variaveis\n",
    "sb.pairplot(boston_data, diag_kind=\"kde\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlação\n",
    "correlation_data = boston_data.corr()\n",
    "correlation_data.style.background_gradient(cmap='coolwarm', axis=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estatistica descitiva\n",
    "stats = boston_data.describe()\n",
    "boston_stats = stats.transpose()\n",
    "boston_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecionar colunas requeridas \n",
    "X_data = boston_data[[i for i in boston_data.columns if i not in ['MEDV']]]\n",
    "Y_data = boston_data[['MEDV']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinamento e teste\n",
    "training_features , test_features ,training_labels, test_labels = train_test_split(X_data , Y_data , test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('No. de filas no Treinamento: ', training_features.shape[0])\n",
    "print('No. de filas no Teste: ', test_features.shape[0])\n",
    "print()\n",
    "print('No. de colunas no Treinamento: ', training_features.shape[1])\n",
    "print('No. de colunas no Teste: ', test_features.shape[1])\n",
    "print()\n",
    "print('No. de filas nas rótulos de Treinamento: ', training_labels.shape[0])\n",
    "print('No. de filas nos rótulos de Teste: ', test_labels.shape[0])\n",
    "print()\n",
    "print('No. de colunas nos rótulos de Treinamento: ', training_labels.shape[1])\n",
    "print('No. de colunas nos rótulos de Teste: ', test_labels.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizado os dados\n",
    "def norm(x):\n",
    "    stats = x.describe()\n",
    "    stats = stats.transpose()\n",
    "    return (x - stats['mean']) / stats['std']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normaliza\n",
    "normed_train_features = norm(training_features)\n",
    "print(normed_train_features)\n",
    "normed_test_features = norm(test_features)\n",
    "#print(normed_test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contruir o pipeline para modelo do TensorFlow\n",
    "def feed_input(features_dataframe, target_dataframe,num_of_epochs=10, shuffle=True, batch_size=32):\n",
    "    #\n",
    "    def input_feed_function():\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((dict(features_dataframe), target_dataframe))\n",
    "        if shuffle:\n",
    "            dataset = dataset.shuffle(2000)\n",
    "        dataset = dataset.batch(batch_size).repeat(num_of_epochs)\n",
    "        return dataset\n",
    "    #\n",
    "    return input_feed_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usando a função definida acima\n",
    "train_feed_input = feed_input(normed_train_features,training_labels)\n",
    "train_feed_input_testing = feed_input(normed_train_features,training_labels, num_of_epochs=1, shuffle=False)\n",
    "test_feed_input = feed_input(normed_test_features,test_labels, num_of_epochs=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelando o treiamento\n",
    "feature_columns_numeric = [tf.feature_column.numeric_column(m) for m in training_features.columns]\n",
    "# modelo\n",
    "linear_model = LinearRegressor(feature_columns=feature_columns_numeric, optimizer='RMSProp')\n",
    "#\n",
    "linear_model.train(train_feed_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predições\n",
    "train_predictions = linear_model.predict(train_feed_input_testing)\n",
    "test_predictions = linear_model.predict(test_feed_input)\n",
    "train_predictions_series = pd.Series([p['predictions'][0] for p in train_predictions])\n",
    "test_predictions_series = pd.Series([p['predictions'][0] for p in test_predictions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions_df = pd.DataFrame(train_predictions_series, columns=['predictions'])\n",
    "test_predictions_df = pd.DataFrame(test_predictions_series, columns=['predictions'])\n",
    "#\n",
    "training_labels.reset_index(drop=True, inplace=True)\n",
    "train_predictions_df.reset_index(drop=True, inplace=True)\n",
    "#\n",
    "test_labels.reset_index(drop=True, inplace=True)\n",
    "test_predictions_df.reset_index(drop=True, inplace=True)\n",
    "#\n",
    "train_labels_with_predictions_df = pd.concat([training_labels, train_predictions_df], axis=1)\n",
    "test_labels_with_predictions_df = pd.concat([test_labels,test_predictions_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validação\n",
    "def calculate_errors_and_r2(y_true, y_pred):\n",
    "    mean_squared_err = (mean_squared_error(y_true, y_pred))\n",
    "    root_mean_squared_err = np.sqrt(mean_squared_err)\n",
    "    r2 = round(r2_score(y_true, y_pred)*100,0)\n",
    "    return mean_squared_err, root_mean_squared_err, r2\n",
    "#\n",
    "train_mean_squared_error, train_root_mean_squared_error,train_r2_score_percentage = calculate_errors_and_r2(training_labels, train_predictions_series)\n",
    "#\n",
    "test_mean_squared_error, test_root_mean_squared_error,test_r2_score_percentage = calculate_errors_and_r2(test_labels, test_predictions_series)\n",
    "#\n",
    "print('Dados de Treinamento - Mean Squared Error = ', train_mean_squared_error)\n",
    "print('Dados de Treinamento - Root Mean Squared Error = ', train_root_mean_squared_error)\n",
    "print('Dados de Treinamento - R2 = ', train_r2_score_percentage)\n",
    "print('Dado de Teste - Mean Squared Error = ', test_mean_squared_error)\n",
    "print('Dados de Teste - Root Mean Squared Error = ', test_root_mean_squared_error)\n",
    "print('Dados de Teste - R2 = ', test_r2_score_percentage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contsruir uma Rede Neural com TensorFlow\n",
    "> Dataset **MNIST dataset** [Dataset](http://yann.lecun.com/exdb/mnist/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset usando o Keras\n",
    "(training_images, training_labels), (test_images, test_labels) = ks.datasets.fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizando alguns dados\n",
    "print('Imagenes de Treinamento - Dataset Shape: {}'.format(training_images.shape))\n",
    "print('No. de imagens de Treinamento - Dataset Labels: {}'.format(len(training_labels)))\n",
    "print('Imagens de teste - Dataset Shape: {}'.format(test_images.shape))\n",
    "print('No. de imagens de Teste - Dataset Labels: {}'.format(len(test_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-escala (0-255) -> 0-1\n",
    "training_images = training_images / 255.0\n",
    "test_images = test_images / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usando keras para as diferentes camadas\n",
    "input_data_shape = (28, 28)                                # entrada de 28x28 pixels\n",
    "#\n",
    "hidden_activation_function = 'relu'                        # função de ativação - camada oculta\n",
    "output_activation_function = 'softmax'                     # função de ativação  - camada de saida\n",
    "#\n",
    "nn_model = ks.Sequential()                                 # modelo sequencial\n",
    "#\n",
    "nn_model.add(ks.layers.Flatten(input_shape=input_data_shape, name='Input_layer'))\n",
    "nn_model.add(ks.layers.Dense(32, activation=hidden_activation_function, name='Hidden_layer'))\n",
    "nn_model.add(ks.layers.Dense(10, activation=output_activation_function, name='Output_layer'))\n",
    "nn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# otimizando : SGD, RMSprop, Adam, Adadelta, Adagrad, Adamax, Nadam, Ftrl\n",
    "#\n",
    "optimizer = 'adam'\n",
    "loss_function = 'sparse_categorical_crossentropy'\n",
    "metric = ['accuracy']\n",
    "#\n",
    "nn_model.compile(optimizer=optimizer, loss=loss_function, metrics=metric)    # compilar\n",
    "#\n",
    "nn_model.fit(training_images, training_labels, epochs=10)                    # ajustar ao modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avaliação do treinamento\n",
    "training_loss, training_accuracy = nn_model.evaluate(training_images, training_labels)\n",
    "print('Acurácia dos dados de Treinamento {}'.format(round(float(training_accuracy),2)*100),'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avaliação do teste\n",
    "test_loss, test_accuracy = nn_model.evaluate(test_images,test_labels)\n",
    "print('Acurácia dos dados de Teste {}'.format(round(float(test_accuracy),2)*100),'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usando o tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "import datetime, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset\n",
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "#\n",
    "(x_train, y_train),(x_test, y_test) = fashion_mnist.load_data()\n",
    "#\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criando modelo simples\n",
    "def create_model():\n",
    "    #\n",
    "    return tf.keras.models.Sequential([\n",
    "            tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "            tf.keras.layers.Dense(512, activation='relu'),\n",
    "            tf.keras.layers.Dropout(0.2),\n",
    "            tf.keras.layers.Dense(10, activation='softmax')\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# treinamento do modelo\n",
    "def train_model():\n",
    "    #\n",
    "    model = create_model()\n",
    "    model.compile(optimizer='adam',\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "    #\n",
    "    logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
    "    #\n",
    "    model.fit(x=x_train, y=y_train, epochs=5, \n",
    "            validation_data=(x_test, y_test), \n",
    "            callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iniciar o tensorboar\n",
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %tensorboard --logdir logs usar antes do treinamento\n",
    "#!kill 8104"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Fundamentos para Ciência Dados &copy; Copyright 2021, Sergio Serra & Jorge Zavaleta"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
