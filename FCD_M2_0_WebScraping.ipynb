{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![PPGI_UFRJ](imagens/ppgi-ufrj.png)\n",
    "# Fundamentos de Ciência de Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "[![DOI](https://zenodo.org/badge/335308405.svg)](https://zenodo.org/badge/latestdoi/335308405)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# PPGI/UFRJ 2020.3/2022.2\n",
    "## Prof Sergio Serra e Jorge Zavaleta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Raspagem de dados com Python e BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> A biblioteca BeautifulSoup é amplamente usada para analisar documentos HTML, XML ou outras linguagens de marcação  e extrair dados e gerar arquivos de dados mais claramente estruturados, isso é o que se chama de ***Web Scraping ou Raspagem de Dados***. \n",
    "\n",
    "![wst](imagens/web_scrap.png)\n",
    "\n",
    "> Em geral se usa essa técnica  para coletar informações de um website que não possuia API. Para compreenssão de alguns conceitos nessa aula é importante ter conhecimentos mínimos de HTML tree structure.\n",
    "\n",
    "\n",
    "### Leituras complementares\n",
    "> -  Básico de HTML-BeautifulSoap Aqui --> [Guia para iniciantes]('https://www.vooo.pro/insights/guia-para-iniciantes-de-web-scraping-em-python-usando-beautifulsoup/')\n",
    "> -  Documentação oficial do BeautifulSoap --> [BeatifulSoap]('https://www.crummy.com/software/BeautifulSoup/bs4/doc/')\n",
    "\n",
    "> A base desse exercício é o exemplo do MIT: [Python Scraping]('http://duspviz.mit.edu/tutorials/python-scraping/') e [MIT]('http://duspviz.mit.edu/_assets/data/county_housing_stats.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importação das Bibliotecas Request e BeautifulSoap  \n",
    "\n",
    "A primeira coisa que precisamos fazer para examinar uma página da web é fazer o download dela. Podemos fazer o download de páginas usando a biblioteca **requests** do Python. \n",
    "\n",
    "A biblioteca requests fará uma solicitação GET para um servidor da web, que fará o download do conteúdo HTML de uma determinada página da Web. Existem vários tipos diferentes de solicitações que podemos fazer usando requests, GET é apenas uma.\n",
    "\n",
    "A biblioteca **BeautifulSoup** é usada para analisar o documento e extrair os textos a partir do elementos do HTML. Primeiro, precisamos importar a biblioteca e criar uma instância da classe BeautifulSoup para analisar um documento pretendido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests       # Biblioteca Requests\n",
    "import bs4            # Biblioteca BeautifulSoap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os próximos passos são testes de  outputs de uma requisição do site que desejamos \"raspar\" os dados. \n",
    "\n",
    "Utilizaremos  **http://duspviz.mit.edu/_assets/data/county_housing_stats.html**  para criar um arquivo texto (CSV) com  dados livre de formatação\n",
    "\n",
    "Analise a imagem do site exemplo\n",
    "\n",
    "![tabela](imagens/ws_tabela.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing output Requests\n",
    "response = requests.get('http://duspviz.mit.edu/_assets/data/county_housing_stats.html')\n",
    "print(response.text)                         # Print o output da página HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testando BeautifulSoup\n",
    "\n",
    "> #### Inspecionar um WebSite\n",
    ">> Os browsers já têm incorporado um \"inspecionador\", que é ativado ao clicar sobre uma página com botão direito do mouse.\n",
    "\n",
    "![Inspecionar](imagens/ins_chrome0.png)\n",
    "\n",
    "\n",
    "> #### Prettify \n",
    ">> Método do BS  para visualizar a estrutura separada da página HTML\n",
    "\n",
    ">Em linhas gerais o código a seguir irá analisar response.text criado pelo objeto BeautifulSoup e atribuido-o a soup. \n",
    "O argumento html.parser indica que queremos fazer a análise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing output BS\n",
    "\n",
    "soup = bs4.BeautifulSoup(response.text, \"html.parser\")\n",
    "print(soup.prettify())                        # Print o output usando a função 'prettify' \n",
    "\n",
    "# Using the Beautiful Soup prettify() function,\n",
    "# we can print the page to see the code printed in a \n",
    "# readable and legible manner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Navegando na estrutura de dados gerada pelo Prettify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the title \n",
    "# Access the title element\n",
    "#soup.titlelement\n",
    "soup.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the content of the title element\n",
    "soup.title.string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access data in the first 'p' tag\n",
    "soup.p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access data in the first 'a' tag\n",
    "soup.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve all links in the document (note it returns an array)\n",
    "soup.find_all('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve elements by class equal to link using the attributes argument\n",
    "soup.findAll(attrs={'class' : 'link'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve a specific link by ID\n",
    "soup.find(id=\"link3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access Data in the table (note it returns an array)\n",
    "soup.find_all('td')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carregando os Dados em Arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A maneira mais fácil de acessar elementos (dados) da página é gravá-los em um arquivo temporário e manipulá-los e salvá-los como objetos. \n",
    "\n",
    "Observe que os dados da página a ser raspada são organizados em \"counties\" e várias colunas com números. Logo, salvá-los em arrays é maneira mais lógica e fácil de trabalhar com os dados futuramente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Load\n",
    "data = soup.findAll(attrs={'class':'name'})\n",
    "print(data[0].string)\n",
    "print(data[1].string)\n",
    "print(data[2].string)\n",
    "print(data[3].string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para evitar esse estrutura \"feiosa\", podemos substituir por um loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Better data Load\n",
    "data = soup.findAll(attrs={'class':'name'})\n",
    "for i in data:\n",
    "    print(i.string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No entanto, esse array só traria as  \"counties\", abaixo o loop completo com carga de todos os dados no array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A professional-look Loop data[x]\n",
    "# where x is location in the array. \n",
    "#Remember that in Python, arrays start at 0, so place 1 in a Python array is \n",
    "# actually called by using a 0,  and place 8 would be called by a 7.\n",
    "\n",
    "data = soup.findAll(attrs={'class':['name','fips','tot-pop','median-income','no-housing-units','med-home-val','owner-occupied','house-w-debt','house-wo-debt']})\n",
    "for i in data:\n",
    "    print(i.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apenas um teste\n",
    "print(data[0].text)\n",
    "print(data[1].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apenas um teste\n",
    "print(data[0].string)\n",
    "print(data[1].string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criando e executando o arquivo de script do BeatutifuSoap (.py) para raspagem de dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obeserve que podemos fazer a raspagem executando diretamente um script python através da linha de comandos, pois muitas vezes é um processo demorado. No caso dessa aula, dividimos o script em duas céluas para serem execuradas ao vivo. \n",
    "\n",
    "Caso deseje gerar um script .py, copie o conteúdo das duas próximas céluas em um único arquivo e execute diretamente no Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Opcionalmente pode ser executado dentro do Jupyter\n",
    "import requests\n",
    "import bs4\n",
    "\n",
    "# load and get the page from the website\n",
    "response = requests.get('http://duspviz.mit.edu/_assets/data/county_housing_stats.html')\n",
    "\n",
    "# create the soup\n",
    "soup = bs4.BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# find all the tags with class city or number\n",
    "data = soup.findAll(attrs={'class':['name','fips','tot-pop','median-income','no-housing-units','med-home-val','owner-occupied','house-w-debt','house-wo-debt']})\n",
    "\n",
    "# print 'data' to console optionally\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gravando dados em um arquivo texto (csv) usando um loop simples. A Lógica é mais ou menos a mesma em vários casos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No pseudo-codigo:\n",
    "\n",
    ">- 1 - Abra o arquivo de saida para escrever e apendar os dados raspados \n",
    ">- 2 - Defina os parâmetros do loop (While)\n",
    ">- 3 - Escreva os cabeçalhos (headers)\n",
    ">- 4 - Execute o Loop que irá produzir os elementos do array no arquivo \n",
    ">- 5 - Quando completar o Loop, fecha o arquivo \n",
    ">- 6 - Agora é só analisar os dados raspados :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Opcionalmente pode ser executado dentro do Jupyter\n",
    "\n",
    "f = open('data/country_data.csv','a')           # open new file, make sure path to your data file is correct\n",
    "\n",
    "p = 0                                     # initial place in array\n",
    "l = len(data)-1                           # length of array minus one\n",
    "\n",
    "f.write(\"County, State, FIPS Code, Total Pop, Median Income ($), No. of Housing Units, Median Home Value ($), No. of Owner Occupied Housing Units, No. of Owner Occ. Housing Units with Debt, No. of Owner Occ. Housing Units without Debt\\n\") \n",
    "                                          #write headers\n",
    "\n",
    "while p < l:                              # while place is less than length\n",
    "    f.write(data[p].string + \", \")        # write county and add comma\n",
    "    p = p + 1                             # increment\n",
    "    f.write(data[p].string + \", \")        # write FIPS and add comma\n",
    "    p = p + 1                             # increment\n",
    "    f.write(data[p].string + \", \")        # write Total Pop and add comma\n",
    "    p = p + 1                             # increment\n",
    "    f.write(data[p].string + \", \")        # write Median Income and add comma\n",
    "    p = p + 1                             # increment\n",
    "    f.write(data[p].string + \", \")        # write No. of Housing Units and add comma\n",
    "    p = p + 1                             # increment\n",
    "    f.write(data[p].string + \", \")        # write Median Home Value and add comma\n",
    "    p = p + 1                             # increment\n",
    "    f.write(data[p].string + \", \")        # write No. of Owner Occupied Housing Units and add comma\n",
    "    p = p + 1                             # increment\n",
    "    f.write(data[p].string + \", \")        # write No. of Owner Occ. Housing Units with Debt and add comma\n",
    "    p = p + 1                             # increment\n",
    "    f.write(data[p].string + \"\\n\")        # write No. of Owner Occ. Housing Units without Debt and line break\n",
    "    p = p + 1                             # increment\n",
    "    \n",
    "f.close()                                 # close file\n",
    "print(\"THE END\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abra o arquivo texto county_data.txt e visualizar os dados raspados do site  'http://duspviz.mit.edu/_assets/data/county_housing_stats.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Não se esqueça de inserir seus métodos de Data Provenance na sua raspagem!\n",
    "\n",
    "Eles irão lhe ajudar a assegurar a rastreabilidade dos dados e do processo\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outras Bibliotecas para fazer raspagem\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UrlLib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Urllib** no Python 3 é uma coleção de módulos que se pode usar para trabalhar com URLs. A versão atual do urllib é composta dos seguintes módulos:\n",
    "\n",
    "- urllib.request\n",
    "- urllib.error\n",
    "- urllib.parse\n",
    "- urllib.rebotparser\n",
    "\n",
    "Para maiores informações - https://docs.python.org/3/library/urllib.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![selenium](imagens/ws_sel.png)\n",
    "\n",
    "O conteúdo no qual se tem interesse nem sempre é de fácil acesso. Pode ser necessário interagir com o site para encontrá-lo: seja descendo uma página, clicando em menus e imagens. Ou talvez os dados desejados sejam apresentados utilizando JavaScript. Em todos esses casos, uma requisição do código fonte da página não é o bastante para encontrar as informações almejadas. \n",
    "\n",
    "O **Selenium** é usado nesses casos, é uma ferramenta que permite automatizar um browser (webdrivers), e realizar inúmeros tipos de ações com eles. Sua curva de aprendizado é mais íngrime, e por de fato abrir e controlar um browser, sua atuação é um pouco mais lenta. \n",
    "\n",
    "Por isso, seu uso não é recomendado quando os dados de interesse são de fácil acesso – a biblioteca *requests* dará conta do recado de forma mais rápida e eficiente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instalação\n",
    "> Guia de instalação - [Instalação]('https://selenium-python.readthedocs.io/installation.html')\n",
    "\n",
    "> Instalar os drivers correspondentes ao browser usado.\n",
    "\n",
    "> Adicionar **PATH** dos drivers instalados ao path do sistema.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para maiores informações - https://www.selenium.dev/\n",
    "\n",
    "Tente executar o script abaixo, foi adaptado de http://kevincsong.com/Scraping-stats.nba.com-with-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "import json\n",
    "import time\n",
    "\n",
    "# Pegar conteúdo HTML a partir da URL do site com as estatísticas da NBA\n",
    "url = \"https://stats.nba.com/players/traditional/?PerMode=Totals&Season=2019-20&SeasonType=Regular%20Season&sort=PLAYER_NAME&dir=-1\"\n",
    "top10ranking = {}\n",
    "\n",
    "rankings = {\n",
    "    '3pontos': {'field': 'FG3M', 'label': '3PM'},\n",
    "    'pontos': {'field': 'PTS', 'label': 'PTS'},\n",
    "    'assistencias': {'field': 'AST', 'label': 'AST'},\n",
    "    'rebotes': {'field': 'REB', 'label': 'REB'},\n",
    "    'roubadas': {'field': 'STL', 'label': 'STL'},\n",
    "    'bloqueios': {'field': 'BLK', 'label': 'BLK'},\n",
    "}\n",
    "\n",
    "def buildrank(type):\n",
    "\n",
    "    field = rankings[type]['field']\n",
    "    label = rankings[type]['label']\n",
    "\n",
    "    driver.find_element_by_xpath(\n",
    "        f\"//div[@class='nba-stat-table']//table//thead//tr//th[@data-field='{field}']\").click()\n",
    "\n",
    "    #Hack pra dar tempo pro click na janelinha de privacy anti-scraping  \n",
    "    time.sleep(3)      \n",
    "    \n",
    "    element = driver.find_element_by_xpath(\n",
    "        \"//div[@class='nba-stat-table']//table\")\n",
    "    html_content = element.get_attribute('outerHTML')\n",
    "\n",
    "    # Parsear o conteúdo HTML - BeaultifulSoup\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    table = soup.find(name='table')\n",
    "\n",
    "    # Estruturar conteúdo em um Data Frame - Pandas\n",
    "    df_full = pd.read_html(str(table))[0].head(50)\n",
    "    df = df_full[['Unnamed: 0', 'PLAYER', 'TEAM', label]]\n",
    "    df.columns = ['posicao', 'jogador', 'time', 'total']\n",
    "\n",
    "    # Transformar os Dados em um Dicionário de dados próprio\n",
    "    return df.to_dict('records')\n",
    "\n",
    "\n",
    "option = Options()\n",
    "option.headless = True\n",
    "\n",
    "# Path do geckodriver, necessário para funcionar, outra alyernativa é colocar o .exe no path do sistema\n",
    "driver = webdriver.Firefox(executable_path=r'C:\\selenium_drivers\\geckodriver.exe')\n",
    "#driver = webdriver.Chrome(executable_path=r'C:\\selenium_drivers\\chromedriver.exe')\n",
    "\n",
    "driver.get(url)\n",
    "driver.implicitly_wait(10)                     # aguarda 10 s entre os clicks no site da NBA\n",
    "\n",
    "for k in rankings:\n",
    "    top10ranking[k] = buildrank(k)\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "# Converter e salvar em um arquivo JSON\n",
    "with open('ranking.json', 'w', encoding='utf-8') as jp:\n",
    "    js = json.dumps(top10ranking, indent=4)\n",
    "    jp.write(js)\n",
    "    \n",
    "print(\"END GAME\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrapy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![scrapy](imagens/ws_ws.png)\n",
    "**Scrapy** é mais do que uma biblioteca, é um framework que permite não apenas extrair a informação de uma página, mas navegar por diversas páginas de forma sistemática e automatizada. \n",
    "\n",
    "Esse tipo de técnica recebe o nome de rastreador de rede, ou web crawler. Fornecendo uma lista de páginas inicias (seeds) que devem ser visitadas, a ferramenta é capaz de identificar todas as outras ligações destas páginas para outras – e então visitar essas novas páginas, e assim por diante. \n",
    "\n",
    "**Scrapy** é uma ferramenta poderosa quando se tem interesse em navegar um site, ou apenas uma parte dele ou por completo.\n",
    "\n",
    "Para maiores informações - https://scrapy.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "page = requests.get(\"http://www.ufrj.br\")\n",
    "page.status_code\n",
    "\n",
    "page.content\n",
    "soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "\n",
    "soup.title\n",
    "#soup.title.text\n",
    "\n",
    "#soup.findAll('a')\n",
    "#soup.find('a')\n",
    "\n",
    "#link_list = [l.get('href') for l in soup.findAll('a')]\n",
    "# print(link_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Fudamentos para Ciência Dados &copy; Copyright 2020-2022, Sergio Serra & Jorge Zavaleta"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
